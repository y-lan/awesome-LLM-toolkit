## Models


- [sentence transformers](https://www.sbert.net/docs/pretrained_models.html#model-overview): [fine-tune](https://www.sbert.net/docs/training/overview.html)
  - [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)
  - [GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval](https://arxiv.org/abs/2112.07577) & [Domain Adaptation
](https://www.sbert.net/examples/domain_adaptation/README.html#adaptive-pre-training)
- [mmarco-mMiniLMv2-L12-H384-v1](https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1): multilingual cross-encoder model trained on MMARCO dataset
- [ColBERTv2](https://github.com/stanford-futuredata/ColBERT): 
- [instructor-xl](https://huggingface.co/hkunlp/instructor-xl): EN, 768D, [Paper](https://arxiv.org/abs/2212.09741), use one-sentence instruction to generate task-specific embedding
- OpenAI Embedding: [Paper: Text and Code Embeddings by Contrastive Pre-Training>](https://arxiv.org/abs/2201.10005)



## Datasets

- [Cohere Wikipedia](https://txt.cohere.com/embedding-archives-wikipedia/)
- [unicamp-dl/mmarco](https://huggingface.co/datasets/unicamp-dl/mmarco/viewer/japanese/train), [Github](https://github.com/unicamp-dl/mMARCO)
- [STSb Multi MT](https://huggingface.co/datasets/stsb_multi_mt)
  - [jSTS](https://github.com/yahoojapan/JGLUE/tree/main/datasets/jsts-v1.1)
