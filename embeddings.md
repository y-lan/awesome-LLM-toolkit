## Models


- [sentence transformers](https://www.sbert.net/docs/pretrained_models.html#model-overview): [fine-tune](https://www.sbert.net/docs/training/overview.html)
  - [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)
- [mmarco-mMiniLMv2-L12-H384-v1](https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1): multilingual cross-encoder model trained on MMARCO dataset
- [ColBERTv2](https://github.com/stanford-futuredata/ColBERT): 
- [instructor-xl](https://huggingface.co/hkunlp/instructor-xl): EN, 768D, [Paper](https://arxiv.org/abs/2212.09741), use one-sentence instruction to generate task-specific embedding
- OpenAI Embedding: [Paper](https://arxiv.org/abs/2201.10005)



## Datasets

- [Cohere Wikipedia](https://txt.cohere.com/embedding-archives-wikipedia/)
- [unicamp-dl/mmarco](https://huggingface.co/datasets/unicamp-dl/mmarco/viewer/japanese/train), [Github](https://github.com/unicamp-dl/mMARCO)
